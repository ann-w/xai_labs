{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable Artifical Intelligence (XAI) - Large Language Models \n",
    "\n",
    "___\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "\n",
    "Machine learning (ML) systems are increasingly being integrated across various domains, from cars that drive themselves to smart assistants for improved user interactions. The widespread use of ML in complex applications has increased interest in creating systems that are not only high-performing but also safe, fair, and reliable. However, unlike straightforward performance metrics such as accuracy, these additional criteria are often challenging to quantify. For instance, it's difficult to list all necessary safety checks for a semi-autonomous vehicle or to fully ensure a credit scoring system is free from bias. In these situations, being able to understand how an ML system thinks becomes very important. If an ML system can explain its reasoning, we can then assess whether its logic aligns with these essential standards.\n",
    "\n",
    "This is where Explainable AI (XAI) becomes invaluable. For data scientists and researchers, it's a tool to uncover biases and errors, to improve the system's performance and fairness. For end users and consumers, XAI fosters trust and acceptance by demystifying AI decisions, making them clear and understandable. The ability to peek under the hood of AI systems and understand their thought processes is a necessity, especially with privacy and transparency laws like GDPR highlighting the importance of clarity in AI-driven decisions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**resources**\n",
    "\n",
    "- Using captum to explain LLM, metapaper: https://aclanthology.org/2023.nlposs-1.19.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Finetuning paradigm\n",
    "\n",
    "## Local Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Perturbation based method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below is the example provided by the meta paper:\n",
    "from captum . attr import FeatureAblation , LLMAttribution , TextTemplateFeature\n",
    "fa = FeatureAblation ( model )\n",
    "llm_attr = LLMAttribution ( fa , tokenizer )\n",
    "inp = TextTemplateFeature (\n",
    "# the text template\n",
    "\"{} lives in {}, {} and is a {}. {} personal interests include \",\n",
    "# the values of the features\n",
    "[\" Dave \", \" Palm Coast \", \"FL\", \" lawyer \", \" His\"],\n",
    "# the reference baseline values of the features\n",
    "baselines =[\" Sarah \", \" Seattle \", \"WA\", \" doctor \", \"Her\"],\n",
    ")\n",
    "llm_attr . attribute ( inp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assignment: perturb each of the word to see when the sentiment changes\n",
    "# todo: see if there is a way to quantify each of the words to the output prediction, like coefficients.\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Specify the model name\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Load a pre-trained sentiment analysis model with the specified model name\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "\n",
    "def perturb_input(sentence, word_to_remove):\n",
    "    \"\"\"\n",
    "    Removes a specified word from the sentence and returns the modified sentence.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    modified_words = [word for word in words if word.lower() != word_to_remove.lower()]\n",
    "    return ' '.join(modified_words)\n",
    "\n",
    "def analyze_sentiment_change(original_sentence, word_to_remove):\n",
    "    \"\"\"\n",
    "    Analyzes the change in sentiment when a specific word is removed from the sentence.\n",
    "    \"\"\"\n",
    "    # Original sentiment\n",
    "    original_sentiment = nlp(original_sentence)[0]\n",
    "    \n",
    "    # Modified sentence after removing the word\n",
    "    modified_sentence = perturb_input(original_sentence, word_to_remove)\n",
    "    \n",
    "    # Sentiment of the modified sentence\n",
    "    modified_sentiment = nlp(modified_sentence)[0]\n",
    "    \n",
    "    print(f\"Original sentence: {original_sentence}\")\n",
    "    print(f\"Sentiment: {original_sentiment}\")\n",
    "    print(f\"\\nModified sentence: {modified_sentence}\")\n",
    "    print(f\"Sentiment: {modified_sentiment}\")\n",
    "\n",
    "# Example usage\n",
    "original_sentence = \"The movie was surprisingly good despite its slow start.\"\n",
    "word_to_remove = \"good\"\n",
    "analyze_sentiment_change(original_sentence, word_to_remove)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Gradient based method\n",
    "\n",
    "\n",
    "- [Integrated gradients](https://captum.ai/tutorials/IMDB_TorchText_Interpret)\n",
    "\n",
    "- [Medium article](https://medium.com/@CVxTz/add-interpretability-to-your-nlp-model-the-easy-way-using-captum-ec56f538f746) with [associated notebook](https://colab.research.google.com/drive/1SKitCFjbiZ3k7eL3UfMuKOISKmD_b9bc?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Surrogate Models\n",
    "\n",
    "- [shap](https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/text.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model.eval()\n",
    "\n",
    "# Define a prediction function that takes texts and returns probability scores\n",
    "def predict_proba(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Apply softmax to logits to get probabilities\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    # Convert to numpy array and return\n",
    "    return probabilities.numpy()\n",
    "\n",
    "# Create a LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=[\"Negative\", \"Positive\"])\n",
    "\n",
    "# Example text to explain\n",
    "input_text = \"The movie was surprisingly good despite its slow start.\"\n",
    "\n",
    "# Generate explanation with LIME\n",
    "exp = explainer.explain_instance(input_text, predict_proba, num_features=6, labels=[1])\n",
    "\n",
    "# Show explanation\n",
    "exp.show_in_notebook(text=input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP\n",
    "import nlp\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import shap\n",
    "\n",
    "# load a BERT sentiment analysis model\n",
    "tokenizer = transformers.DistilBertTokenizerFast.from_pretrained(\n",
    "    \"distilbert-base-uncased\"\n",
    ")\n",
    "model = transformers.DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ").cuda()\n",
    "\n",
    "\n",
    "# define a prediction function\n",
    "def f(x):\n",
    "    tv = torch.tensor(\n",
    "        [\n",
    "            tokenizer.encode(v, padding=\"max_length\", max_length=500, truncation=True)\n",
    "            for v in x\n",
    "        ]\n",
    "    ).cuda()\n",
    "    outputs = model(tv)[0].detach().cpu().numpy()\n",
    "    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n",
    "    val = sp.special.logit(scores[:, 1])  # use one vs rest logit units\n",
    "    return val\n",
    "\n",
    "\n",
    "# build an explainer using a token masker\n",
    "explainer = shap.Explainer(f, tokenizer)\n",
    "\n",
    "# explain the model's predictions on IMDB reviews\n",
    "imdb_train = nlp.load_dataset(\"imdb\")[\"train\"]\n",
    "shap_values = explainer(imdb_train[:10], fixed_context=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from meta paper captum\n",
    "from captum . attr import ShapleyValueSampling , LLMAttribution , TextTemplateFeature ,\n",
    "ProductBaselines\n",
    "svs = ShapleyValueSampling ( model )\n",
    "baselines = ProductBaselines (\n",
    "{\n",
    "(\" name \", \" pronoun \"): [(\" Sarah \", \"Her\") , (\" John \", \"His\")],\n",
    "\" city \": [\" Seattle \", \" Boston \"],\n",
    "\" state \": [\"WA\", \"MA\"],\n",
    "\" occupation \": [\" doctor \", \" engineer \", \" teacher \", \" technician \", \" plumber \"],\n",
    "}\n",
    ")\n",
    "llm_attr = LLMAttribution ( svs , tokenizer )\n",
    "inp = TextTemplateFeature (\n",
    "\"{ name } lives in { city }, { state } and is a { occupation }. { pronoun } personal\n",
    "interests include \",\n",
    "{\" name \":\" Dave \", \" city \": \" Palm Coast \", \" state \": \"FL\", \" occupation \":\" lawyer \", \"\n",
    "pronoun \":\" His\"},\n",
    "baselines = baselines ,\n",
    ")\n",
    "attr_result = llm_attr . attribute ( inp , target =\" playing golf , hiking , and cooking .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Decomposition method\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from captum.attr import LayerLRP\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Tokenize the input text\n",
    "input_text = \"The movie was surprisingly good despite its slow start.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "\n",
    "# Initialize LayerLRP with the last layer of the DistilBERT transformer\n",
    "layer_lrp = LayerLRP(model.distilbert.transformer.layer[-1], model.distilbert.embeddings)\n",
    "\n",
    "# Before applying LRP, we need to ensure gradients can flow through the embedding layer\n",
    "model.distilbert.embeddings.requires_grad_(True)\n",
    "\n",
    "# Compute attributions using LRP\n",
    "attributions = layer_lrp.attribute(input_ids)\n",
    "\n",
    "# Process and visualize your attributions as needed\n",
    "# Here we sum the attributions across the embedding dimension and print them\n",
    "attributions_sum = attributions.sum(dim=-1).squeeze(0)\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "for token, attribution in zip(tokens, attributions_sum.cpu().detach().numpy()):\n",
    "    print(f\"{token}: {attribution}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from captum.attr import Saliency\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Tokenize the input text\n",
    "input_text = \"The movie was surprisingly good despite its slow start.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "\n",
    "# Ensure input_ids require gradients\n",
    "input_ids.requires_grad = True\n",
    "\n",
    "# Define a forward function that returns the model's logits\n",
    "def forward_func(input_ids):\n",
    "    return model(input_ids).logits\n",
    "\n",
    "# Initialize Saliency using the forward function\n",
    "saliency = Saliency(forward_func)\n",
    "\n",
    "# Compute the saliency scores for the input IDs with respect to the target class (positive sentiment)\n",
    "# Assuming the positive class index is 1\n",
    "attributions = saliency.attribute(input_ids, target=1)\n",
    "\n",
    "# Process and visualize the attributions as before\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
